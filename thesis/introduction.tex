\chapter{Introduction}

With the explosive growth in data-driven applications across the cloud
and edge, vast quantities of data are continuously generated by sources
such as Internet-of-Things devices, mobile applications, and servers
today.
%
These data are processed by a diverse range of applications and
services in the cloud to gain insights and drive application logic.
%
Data
processing occurs in various contexts, such as ad-hoc analysis of
collected data in batches and real-time monitoring and processing as it
arrives in streaming dataflows.

Cloud services receive raw data and events from the sources mentioned
above.
%
Streaming engines such as Spark~\cite{spark} and Trill~\cite{trill} process
them, and a state management system ingests this processed data.
%
Offline
queries analyze this data to train and update prediction models, analyze
user behavior, generate device crash reports, and much more.
%
Hence, this
state management system (or store) is the crucial piece in these
analytics pipelines; it serves as a focal point for massive numbers of
events and real-time queries over the aggregated results.

The last decade of computer systems research has yielded efficient
stores with throughput and access latency thousands
of times better than conventional stores.
%
Today,
these systems can execute millions of operations per second
with latencies
of 5~\us or less~\cite{mica,ramcloud,farm-2014}.
%
% storage systems have while preserving their performance benefits.
%
These gains come from careful attention to detail in dispatch and
request processing,
so these systems often start with stripped-down designs to
achieve their performance goals.

However, for these low-latency stores to be practical in the long-term,
they must evolve to include many of the features that conventional data
center and cloud storage systems have while preserving their performance
benefits.
%
One key feature is the ability to reconfigure and
(re)distribute data in response to load imbalances and failures (which
frequently occur in practice).
%
A second key feature is retaining
performance and reconfigurability in the cloud, where networking stacks
have been historically slow, and multiple tenants share resources.
%
A third key feature is to support a diverse set of such tenants with
varying access patterns, data models, and performance requirements.

\section{Reconfiguration and Redistribution}

When distributing data (records) across a cluster of
machines, hash partitioning records across
servers is often the norm.
%
It is simple, efficient, scalable, and also helps balance load
across servers.
%
Most systems tend to hash records, prepartition them into coarse hash
buckets, and then assign ownership of these buckets to servers in a
cluster.
%
However, coarse prepartitioning has many drawbacks that hurt
performance and utilization.
%
It can lead to severe load imbalances when applications access a small
set of records more often than the rest~\cite{slicer} (also known as
\emph{skew}).
%
It can also lead to high request fan-out when
applications exhibit temporal locality in the records they
access~\cite{rocksteady}.
%
Mechanisms that reconfigure ownership and
redistribute records between servers can help mitigate these drawbacks.

% Flexible and lazy partitioning creates a unique challenge for
% kernel-bypass storage systems.
%
% Once a decision to partition is made, the partition must be quickly
% moved to it’s new home with minimum impact to performance.
%
% Doing so is hard; these
% systems offer latencies as low as 5~\us,
% so even a few cache misses will significantly hurt
% performance.

\section{Preserving Performance in the Cloud}

Several of these stores exploit many-core hardware to
ingest and index events at high rates -- 100 million operations
(Mops) per second (s) per machine~\cite{mica,flexnic,floem,kvdirect}.
%
However, they rely on application-specific
hardware acceleration, making them impossible to deploy on today's cloud
platforms.
%
Furthermore, these systems only store data in DRAM (Dynamic Random
Access Memory), and they do not scale across
machines; adding support to do so without cutting into normal-case performance
is not straightforward.
%
For example, many of them statically partition records across cores to
eliminate cross-core synchronization.
%
Statically partitioning records optimizes normal-case performance, but
it makes concurrent
operations like migration and scale-out impossible; transferring record data
and ownership between machines and cores requires a stop-the-world approach
due to these systems' lack of fine-grained synchronization.

Achieving high throughput while fulfilling all of these requirements on
commodity cloud platforms requires simultaneously solving two key
challenges.
%
First, workloads change over time, and cloud VMs (Virtual Machines) fail, so
systems must tolerate failure and reconfiguration.
%
Doing this without
hurting normal-case performance at 100 Mops/s is hard since even a
single extra server-side cache miss to check record ownership or
reconfiguration status would cut throughput by tens-of-millions of
operations per second.
%
Second, the high CPU (Central Processing Unit) cost of processing incoming
network packets easily dominates in these workloads, especially since,
historically, cloud networking stacks have not supported high data rates
and high efficiency.

\section{Expressive Data Models and Multitenancy}

Since the end of Dennard scaling, disaggregation has become the norm.
%
A high-speed network separates applications into a compute and
storage tier, allowing each tier to be
provisioned, managed, and scaled independently.
%
However, this approach
is beginning to reach its limits.
%
Applications have evolved to become more data-intensive than ever.
%
In addition to good performance, they often require rich and complex
data models such as social graphs, decision trees,
vectors~\cite{fb-memcache,parameter-server}, to name a few.

On the other hand, storage systems have become faster with the help of
kernel-bypass~\cite{ramcloud,farm-txns}, but at the cost of their
interface – typically simple point lookups and updates.
%
When applications use these simple interfaces to implement their data
model, they stall on network round-trips to the
storage tier.
%
Since the actual lookup or update takes only a few
microseconds at the storage server, these round-trips create a
significant
bottleneck, hurting performance and utilization.
%
Therefore, to fully
leverage these fast storage systems, applications will have to reduce
round-trips by pushing compute to them.

Pushing compute to these fast storage systems is
not straightforward.
%
They need to be shared by multiple tenants to maximize utilization,
but the cost for isolating tenants using conventional techniques is too
high.
%
Hardware isolation
requires a context switch that takes approximately
1.5 microseconds on a modern processor~\cite{splinter}.
%
This time
is roughly equal to the amount of time it takes to
fully process an RPC at the storage server, meaning
that conventional isolation can hurt throughput by
a factor of 2.

% Another trend is
% \emph{multi-tenancy}; sharing a resource between many tenants helps improve
% utilization.
%
% However, the granularity (containers and VMs) and density
% (1-2 tenants per core) of sharing in today’s datacenters is insufficient
% to keep these fast storage systems at high utilization.

\section{Contributions}

{\bf
  Low-latency stores adopt simple, stripped-down designs that optimize
  for normal-case performance, and in the process, trade-off
  practicality and cost-effectiveness at cloud scale.
%
  This dissertation shows that this trade-off is unnecessary. Carefully
  leveraging and extending new and existing abstractions for scheduling,
  data sharing, lock-freedom, and isolation will yield feature-rich
  systems that
  retain their primary performance benefits at cloud scale.
}

This dissertation presents the following horizontal and vertical mechanisms
for rapid low-impact reconfiguration, multitenancy, and expressive data
models to make low-latency storage systems more practical and efficient
at cloud-scale:

\begin{enumerate}
\item \textbf{Fast Data Migration:}
The first piece presents \emph{Rocksteady}~\cite{rocksteady},
a horizontal mechanism for rapid reconfiguration and elasticity.
%
Rocksteady is a live migration technique for scale-out
in-memory key-value stores.
%
It balances three competing goals: it
migrates data quickly, it minimizes response time impact, and it
allows arbitrary fine-grained splits.
%
Rocksteady allows a key-value store to defer all repartitioning work
until the moment of migration, giving it precise and timely control for
load balancing.

\item \textbf{Low-cost Coordination:}
The second piece presents \emph{Shadowfax}~\cite{shadowfax},
a system with horizontal and vertical mechanisms that
allow distributed key-value stores to
span DRAM, SSDs (Solid State Drives), and cloud blob storage transparently while serving
130~Mops/s/VM over commodity Azure VMs using conventional Linux TCP
(Transmission Control Protocol).
%
Beyond
high single-VM performance, Shadowfax uses a unique approach to distributed
reconfiguration that avoids server-side record ownership checks
or cross-core coordination during regular operation and migration.

\item \textbf{Extensibility and Multitenancy:}
The final piece presents a system called \emph{Splinter}~\cite{splinter}
that
provides clients with a vertical mechanism
to extend low-latency key-value stores by migrating (pushing) code to them.
%
Designed for
modern multitenant data centers, Splinter allows
tenants to write
fine-grained extensions and push
them to the store at runtime.
%
The core of
Splinter's design relies on type- and memory-safe
extension code to avoid conventional hardware isolation costs.
%
Splinter's approach
still allows for bare-metal execution, avoids data copying across trust
boundaries, and makes granular storage functions that perform less than
a microsecond of computation practical.

\end{enumerate}

\section{Fast Data Migration}

\emph{Rocksteady} is a live migration technique for scale-out
in-memory key-value stores.
%
Built on top of RAMCloud~\cite{ramcloud}, Rocksteady’s insight is to
leverage application skew to speed up data migration while minimizing
the impact on performance.
%
When migrating a partition from a source to a target, it first migrates
ownership of the partition.
%
Doing so moves load on the partition from the source to the target, creating
headroom on the source for migrating data.
%
The target then pulls records from the source
on-demand to keep the partition online.
%
This on-demand process quickly converges because of skew –
most of the requests issued by an application are for a small set of hot
records.

To fully utilize the created headroom, Rocksteady carefully schedules and
pipelines data migration on both the source and target.
%
Migration tasks work over RAMCloud’s hash table in parallel;
doing so keeps the prefetcher happy, improving cache locality.
%
A shared-memory model allows these tasks to be scheduled on any core,
allowing migration to use any idle resources on the source and target.
%
To further speed up migration, Rocksteady delays
replication of migrated data at the target until after migration.
%
Fault tolerance is guaranteed by maintaining a dependency
between the source and target at RAMCloud’s coordinator (called lineage)
during the migration and recovering all data at the source if either
machine crashes.
%
%
The target could have served writes
to the partition since the migration began because of early
ownership transfer.
%
Therefore, recovery must also include the target.
%
Putting all these parts
together results in a protocol that migrates data 100x faster than the
state-of-the-art while maintaining tail latencies 1000x lower.

Overall, Rocksteady's careful attention to ownership, scheduling, and
fault tolerance helps it quickly and safely migrate data with
low impact.
%
Experiments show that it can migrate at 758 MBps
while maintaining tail latency below 250 microseconds; this is
equivalent to migrating a server's worth of data in a matter of minutes,
allowing for quick cluster reconfiguration.
%
Additionally, early ownership transfer and
lineage help improve migration speed by 25\%.
%
These results have
significant implications on system design; fast storage systems can use
Rocksteady as a mechanism to enable flexible, lazy partitioning of
data.

\section{Low Cost Coordination}

\emph{Shadowfax} allows distributed key-value stores to
span DRAM, SSDs, and cloud blob storage transparently.
%
Its unique approach to
distributed reconfiguration avoids cross-core coordination during
regular operation and data migration both in its indexing and network
interactions.
%
In contrast to totally-ordered or stop-the-world approaches used by most
systems, cores in Shadowfax avoid stalling to synchronize with one another, even when
triggering complex operations like scale-out, which require
defining clear before/after points in time among concurrent operations.
%
Instead, each core participating in these operations -- both at clients and
servers -- independently decides a point in an \emph{asynchronous global
cut} that defines a boundary between operation sequences in these complex operations.
%
Shadowfax vertically extends asynchronous cuts from cores within one
process~\cite{faster} to servers
and clients in a cluster.
%
These cuts help coordinate server
and client threads
in Shadowfax's low-coordination data migration.

In addition to reconfiguration, Shadowfax has mechanisms that help it
achieve a throughput of 130~Mops/s/VM over
commodity Azure VMs~\cite{azure}.
%
First, all requests from a client on one machine to Shadowfax are
completely asynchronous throughout Shadowfax's client-side and
server-side network submission/completion paths and servers' indexing and
(SSD and cloud storage) I/O (Input/Output) paths.
%
This approach avoids all client- and server-side stalls due to head-of-line
blocking, ensuring that clients can always generate
requests, and
servers can always process them.
%
Second, instead of partitioning data among cores to avoid synchronization on record
accesses~\cite{hstore,voltdb,mica,seastar}, Shadowfax partitions network
sessions across cores and shares its lock-free hash index and log-structured
record heap among them.
%
This risks contention when some records are hot and frequently
mutated, but this is more than offset by avoiding software-level
intercore request forwarding or routing within server VMs.

Measurements show that Shadowfax can shift load in 17~s to improve system throughput by
10~Mops/s
with little disruption. Compared to the state-of-the-art, it has eight
times better throughput
  (than Seastar+memcached) and scales out six times faster.
%
When scaled to a small cluster, Shadowfax retains its high throughput to
perform 930~Mops/s,
%
which is the highest
reported throughput for a distributed key-value store used for
large-scale data ingestion and indexing.

\section{Extensibility and Multitenancy}

\emph{Splinter} provides clients with a vertical mechanism
to extend low-latency key-value stores by migrating (pushing) code to them.
%
Splinter relies on a type- and memory-safe language for isolation.
%
Tenants push
extensions – a tree traversal, for example – written in the Rust
programming language~\cite{rust} to the system at runtime.
%
Splinter installs
these extensions.
%
Once installed, an extension can
be remotely invoked (executed) by the tenant in a
single round-trip.
%
For applications such as tree traversals that would ordinarily require
round-trips logarithmic in the tree's size, Splinter can
significantly improve both throughput and latency.

In addition to lightweight isolation, Splinter consists of multiple
mechanisms to make pushing computation feasible.
%
It minimizes cross-core synchronization
by maintaining \emph{tenant locality};
%
it routes tenants
to preferred cores at the NIC~\cite{flow-director} itself.
%
Cores
steal work from
their neighbor to combat any resulting load imbalances.
%
Splinter schedules pushed code (an extension) cooperatively; extensions
periodically yield to the storage layer, ensuring that long-running
extensions do not starve short running ones.
%
This
approach is preferred over conventional multitasking using threads
because preempting a thread requires a context switch, making it too
expensive for microsecond timescales.
%
A dedicated watchdog core identifies and blocks uncooperative
extensions.
%
Splinter passes immutable references to extensions to minimize data
copies; the Rust compiler statically verifies these references' lifetime
and safety.
%
With the help of these mechanisms, Splinter can isolate
hundreds of granular tenant extensions per core while serving millions of
operations per second with microsecond latencies.

Overall, Splinter adds extensibility to fast kernel-bypass storage
systems, making it easier for applications to use them.
%
An 800 line Splinter extension implementing Facebook’s TAO graph
model~\cite{tao-2013}
can serve 2.8 million ops/s on eight threads with an average latency of
30 microseconds.
%
A significant fraction of TAO involves only a single
round-trip.
%
Implementing these on the client using normal lookups and
implementing the remaining operations using the extension helps improve
performance to 3.2 million ops/s at the same latency.
%
Therefore, an
approach that combines normal lookups/updates with Splinter’s extensions
is the best for performance.
%
The normal lookups do not incur isolation
overhead (no matter how low), and the extensions reduce the number of
round-trips.
%
In comparison, FaRM’s~\cite{farm-2014} implementation of TAO performs
6.3 million
ops/s on 32 threads with an average latency of 41 microseconds.
%
Splinter’s approach, which performs 0.4 million ops/s per thread,
is competitive with FaRM’s RDMA based approach, which performs 0.2 million
ops/s per thread.
