% \section{Introduction}
\label{ref:intro}

%K Scale-out in-memory stores: fast, low-latency, but not immune to load imbalancing and reconfiguration.
The last decade of computer systems research has yielded efficient scale-out
in-memory stores with throughput and access latency thousands of times better
than conventional stores. Today, even modest clusters of these machines can
execute billions of operations per second with access times of 6~\us or
less~\cite{ramcloud,farm-2014}.  These gains come from careful attention to detail
in dispatch and request processing, so these systems often start with
stripped-down designs to achieve performance goals. For these
systems to be practical in the long-term, they must evolve to include many of the
features that conventional data center and cloud storage systems have
{\em while} preserving their performance benefits.

%K Want operational flexibility of cloud systems, but preserving throughput and latency gains.
To that end, we present {\em Rocksteady}, a fast reconfiguration
system for the RAMCloud scale-out in-memory store. Rocksteady
facilitates cluster
scale-up, scale-down, and load rebalancing with a low-overhead and flexible
approach that allows data to be migrated at arbitrarily fine-grained boundaries and does not
require any normal-case work to partition records.  Our measurements show that
Rocksteady can improve the efficiency of clustered accesses and index
operations by more than 4$\times$: operations that are common in many
real-world, large-scale systems~\cite{fb-memcache,spanner}.  Several works
address the general problem of online (or {\em live}) data migration for
scale-out
stores~\cite{slacker,estore,squall,albatross,zephyr,dynamo,spanner}.
However, hardware trends and the specialized needs of an in-memory
key-value store
make Rocksteady's approach unique:

\begin{enumerate}
\item {\bf Low-latency Access Times:}
RAMCloud services requests in 6~\us, and predictable, low-latency operation is
its primary benefit.  Rocksteady's focus is on \nnnth{}-percentile response
times but with 1,000$\times$ lower response times than other tail latency
focused systems~\cite{dynamo}. For clients with high fan-out requests, even
a millisecond of extra tail latency would destroy client-observed
performance. Migration must have minimum impact on access latency
distributions.

\item {\bf Growing DRAM Storage:}
Off-the-shelf data center machines pack 256~to~512~GB per server
with terabytes coming soon. Migration speeds must
grow along with DRAM capacity for load balancing and reconfiguration to be
practical. Today's migration techniques would take hours
to move a fraction of a single machine's data, making them ineffective for
scale-up and scale-down of clusters.

\item {\bf High Bandwidth Networking:}
Today, fast in-memory stores use 40~Gbps networks with
200~Gbps~\cite{mellanox-cx6} arriving in 2017. Ideally, with data in memory, these
systems would be able to migrate data at full line rate, but there are many
challenges. For example, we find that these network cards (NICs)
struggle with the scattered, fine-grained objects typical to in-memory
stores~(Section~\ref{sec:hw}).  Even with the most straightforward migration
techniques, moving data at line-rate would severely degrade
normal-case request processing.

\end{enumerate}

% \noindent
In short, the faster and less disruptive we can make migration, the more often we can
afford to use it, making it easier to exploit locality and scaling for efficiency gains.

Besides hardware, three aspects of RAMCloud's design affect Rocksteady's
approach; it is a high-availability system, focuses on low-latency
operation, and its servers internally (re)arrange data to optimize memory
utilization and garbage collection. These aspects lead to the following
three design goals for Rocksteady:

\begin{enumerate}
\item {\bf Pauseless:}
RAMCloud must be available at all times~\cite{ramcloud-recovery}, so
Rocksteady can never take tables offline for migration.

\item {\bf Lazy Partitioning:}
For load balancing, servers in most systems
internally pre-partition data
to minimize overhead at migration time~\cite{dynamo,farm-2014}.
Rocksteady rejects this approach for two reasons. First, deferring all
partitioning until migration time lets Rocksteady make partitioning
decisions with full information at hand; a set
of predefined splits never constrains it. Second, DRAM-based storage is
expensive; during
regular operation, RAMCloud's log cleaner~\cite{ramcloud-lsm}
continually
reorganizes data physically in memory to improve utilization and
minimize cleaning costs. Forcing a partitioning on server state
would harm the cleaner's efficiency, which is key to making RAMCloud
cost-effective.

\item {\bf Low Impact With Minimum Headroom:}
Migration increases load on the source and target servers. This increase
is particularly
problematic for the source since RAMCloud might initiate migration to cope with
increasing load. Efficient use of hardware resources is critical during
migration; preserving headroom for rebalancing increases the
system's cost directly.

\end{enumerate}

% \noindent
Four key ideas allow Rocksteady to meet these goals:
\begin{enumerate}
\item {\bf Adaptive Parallel Replay:}
For servers to keep up with fast networks during migration, Rocksteady fully
pipelines and parallelizes all migration phases between the source and
target servers. For example, target servers spread incoming data across idle
cores to speed up index reconstruction, but migration operations yield to
client requests for data to minimize disruption.

\item {\bf Exploit Workload Skew to Create Source-side Headroom:} Rocksteady
  prioritizes migration of hot records. This prioritization
  quickly shifts some load with minimal impact for typical skewed
  workloads, which creates headroom on the
  source to allow faster migration with less disruption.

\item {\bf Lineage-based Fault Tolerance:}
Each RAMCloud server logs updated records in a distributed, striped log,
also kept (once) in-memory to service requests. A server does not know how its
contents will be partitioned during migration, so records are intermixed
in memory and storage. This complicates fault tolerance during
migration: it is expensive to synchronously reorganize on-disk data to move
records from the scattered chunks of one server's log into the scattered chunks
of another's.  Rocksteady takes inspiration from
Resilient Distributed Datasets~\cite{spark}; servers can take dependencies
on portions of each others' recovery logs, allowing them to safely
reorganize storage asynchronously.

\item {\bf Optimization for Modern NICs:}
%Rocksteady is the first data migration mechanism to perform live migrations
%at near-line rate ({\color{red}4~GB/s}) on modern networking hardware, yet it
%preserves SLAs {\color{red} 1,000x} lower than conventional stores.
Fast migration with tight tail latency bounds requires careful attention to
hardware at every point in the design; any ``hiccup'' or extra load results in
latency spikes.
%We profiled modern NICs to
%understand how to best optimize large transfers for the network while
%minimizing impact on host CPU and memory accesses.
%Rocksteady can use kernel-bypass and modern NIC DMA for source-side zero-copy
%of migrated records whenever the hardware supports it;
Rocksteady uses kernel-bypass for low overhead
migration of records;
the result is a fast
transfer with reduced CPU load, reduced memory bandwidth load, and more
stable normal-case performance.

\end{enumerate}

This chapter starts by motivating Rocksteady
(Section~\ref{sec:rmotivation}) and quantifying the
gains it can achieve. Then, it shows why state-of-the-art migration techniques
are insufficient for RAMCloud, including a breakdown of why RAMCloud's simple,
preexisting migration is inadequate
(Section~\ref{sec:bottlenecks}). This chapter then describes Rocksteady's full design
(Section~\ref{sec:new-design}), including its fault tolerance strategy, and
evaluates
its performance with an emphasis on migration speed and tail latency
impact~(Section~\ref{sec:reval}).
Compared to prior approaches, Rocksteady transfers data an order of magnitude
faster ($>$~750~MB/s) with median and tail latencies 1,000$\times$ lower
($<$~40~\us and
250~\us, respectively). In general, Rocksteady's ability to use {\em any}
available core for {\em any} operation is crucial for both tail latency and
migration speed.
